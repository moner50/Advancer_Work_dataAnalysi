{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moner50/Advancer_Work_dataAnalysi/blob/master/Image_Caption_Generator(Pytorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkVm7PDlaCS0",
        "outputId": "b0c4dcf7-ea2e-469c-948d-47900affd93a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2\n",
            "  Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2) (1.3.0)\n",
            "Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m509.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/731.7 MB\u001b[0m \u001b[31m170.3 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.2\n",
        "!pip install torchvision==0.17\n",
        "!pip install nltk==3.7\n",
        "!pip install pycocotools==2.0.7\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import nltk\n",
        "import pycocotools\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "print(nltk.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the data"
      ],
      "metadata": {
        "id": "_GZvXnTkKALO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# linux\n",
        "!apt-get install wget\n",
        "# mac\n",
        "# !brew install wget\n",
        "\n",
        "# create a data directory\n",
        "!mkdir data_dir\n",
        "\n",
        "# download images and annotations to the data directory\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip -P ./data_dir/\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip -P ./data_dir/\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip -P ./data_dir/\n",
        "\n",
        "# extract zipped images and annotations and remove the zip files\n",
        "!unzip ./data_dir/annotations_trainval2014.zip -d ./data_dir/\n",
        "!rm ./data_dir/annotations_trainval2014.zip\n",
        "!unzip ./data_dir/train2014.zip -d ./data_dir/\n",
        "!rm ./data_dir/train2014.zip\n",
        "!unzip ./data_dir/val2014.zip -d ./data_dir/\n",
        "!rm ./data_dir/val2014.zip"
      ],
      "metadata": {
        "id": "LtH43H6caQKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the modules"
      ],
      "metadata": {
        "id": "2tV6haQZKEz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from pycocotools.coco import COCO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.utils.rnn import pack_padded_sequence"
      ],
      "metadata": {
        "id": "Kq_lMw_TaWRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "qx8zMfOkaaoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the vocab class"
      ],
      "metadata": {
        "id": "MBL5Ix6YKL0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab(object):\n",
        "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.w2i = {}\n",
        "        self.i2w = {}\n",
        "        self.index = 0\n",
        "\n",
        "    def __call__(self, token):\n",
        "        if not token in self.w2i:\n",
        "            return self.w2i['<unk>']\n",
        "        return self.w2i[token]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.w2i)\n",
        "    def add_token(self, token):\n",
        "        if not token in self.w2i:\n",
        "            self.w2i[token] = self.index\n",
        "            self.i2w[self.index] = token\n",
        "            self.index += 1\n",
        "\n",
        "def build_vocabulary(json, threshold):\n",
        "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
        "    coco = COCO(json)\n",
        "    counter = Counter()\n",
        "    ids = coco.anns.keys()\n",
        "    for i, id in enumerate(ids):\n",
        "        caption = str(coco.anns[id]['caption'])\n",
        "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "        counter.update(tokens)\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
        "\n",
        "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
        "    tokens = [token for token, cnt in counter.items() if cnt >= threshold]\n",
        "\n",
        "    # Create a vocab wrapper and add some special tokens.\n",
        "    vocab = Vocab()\n",
        "    vocab.add_token('<pad>')\n",
        "    vocab.add_token('<start>')\n",
        "    vocab.add_token('<end>')\n",
        "    vocab.add_token('<unk>')\n",
        "\n",
        "    # Add the words to the vocabulary.\n",
        "    for i, token in enumerate(tokens):\n",
        "        vocab.add_token(token)\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocabulary(json='data_dir/annotations/captions_train2014.json', threshold=4)\n",
        "vocab_path = './data_dir/vocabulary.pkl'\n",
        "with open(vocab_path, 'wb') as f:\n",
        "    pickle.dump(vocab, f)\n",
        "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
        "print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
      ],
      "metadata": {
        "id": "bAc3SOMiacfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shaping up the images"
      ],
      "metadata": {
        "id": "6PTObhJFKjYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_image(image, shape):\n",
        "    \"\"\"Resize an image to the given shape.\"\"\"\n",
        "    return image.resize(shape, Image.LANCZOS)\n",
        "\n",
        "def reshape_images(image_path, output_path, shape):\n",
        "    \"\"\"Reshape the images in 'image_path' and save into 'output_path'.\"\"\"\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "\n",
        "    images = os.listdir(image_path)\n",
        "    num_im = len(images)\n",
        "    for i, im in enumerate(images):\n",
        "        with open(os.path.join(image_path, im), 'r+b') as f:\n",
        "            with Image.open(f) as image:\n",
        "                image = reshape_image(image, shape)\n",
        "                image.save(os.path.join(output_path, im), image.format)\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
        "                   .format(i+1, num_im, output_path))\n",
        "\n",
        "image_path = './data_dir/train2014/'\n",
        "output_path = './data_dir/resized_images/'\n",
        "image_shape = [256, 256]\n",
        "reshape_images(image_path, output_path, image_shape)"
      ],
      "metadata": {
        "id": "UrsoWI8WaeGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the dataset"
      ],
      "metadata": {
        "id": "jT5C4_XjKnM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCocoDataset(data.Dataset):\n",
        "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
        "    def __init__(self, data_path, coco_json_path, vocabulary, transform=None):\n",
        "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
        "\n",
        "        Args:\n",
        "            root: image directory.\n",
        "            json: coco annotation file path.\n",
        "            vocab: vocabulary wrapper.\n",
        "            transform: image transformer.\n",
        "        \"\"\"\n",
        "        self.root = data_path\n",
        "        self.coco_data = COCO(coco_json_path)\n",
        "        self.indices = list(self.coco_data.anns.keys())\n",
        "        self.vocabulary = vocabulary\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
        "        coco_data = self.coco_data\n",
        "        vocabulary = self.vocabulary\n",
        "        annotation_id = self.indices[idx]\n",
        "        caption = coco_data.anns[annotation_id]['caption']\n",
        "        image_id = coco_data.anns[annotation_id]['image_id']\n",
        "        image_path = coco_data.loadImgs(image_id)[0]['file_name']\n",
        "\n",
        "        image = Image.open(os.path.join(self.root, image_path)).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert caption (string) to word ids.\n",
        "        word_tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
        "        caption = []\n",
        "        caption.append(vocabulary('<start>'))\n",
        "        caption.extend([vocabulary(token) for token in word_tokens])\n",
        "        caption.append(vocabulary('<end>'))\n",
        "        ground_truth = torch.Tensor(caption)\n",
        "        return image, ground_truth\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "\n",
        "def collate_function(data_batch):\n",
        "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
        "\n",
        "    We should build custom collate_fn rather than using default collate_fn,\n",
        "    because merging caption (including padding) is not supported in default.\n",
        "    Args:\n",
        "        data: list of tuple (image, caption).\n",
        "            - image: torch tensor of shape (3, 256, 256).\n",
        "            - caption: torch tensor of shape (?); variable length.\n",
        "    Returns:\n",
        "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
        "        targets: torch tensor of shape (batch_size, padded_length).\n",
        "        lengths: list; valid length for each padded caption.\n",
        "    \"\"\"\n",
        "    # Sort a data list by caption length (descending order).\n",
        "    data_batch.sort(key=lambda d: len(d[1]), reverse=True)\n",
        "    imgs, caps = zip(*data_batch)\n",
        "\n",
        "    # Merge images (from list of 3D tensors to 4D tensor).\n",
        "    # Originally, imgs is a list of <batch_size> number of RGB images with dimensions (3, 256, 256)\n",
        "    # This line of code turns it into a single tensor of dimensions (<batch_size>, 3, 256, 256)\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "\n",
        "    # Merge captions (from list of 1D tensors to 2D tensor), similar to merging of images donw above.\n",
        "    cap_lens = [len(cap) for cap in caps]\n",
        "    tgts = torch.zeros(len(caps), max(cap_lens)).long()\n",
        "    for i, cap in enumerate(caps):\n",
        "        end = cap_lens[i]\n",
        "        tgts[i, :end] = cap[:end]\n",
        "    return imgs, tgts, cap_lens\n",
        "\n",
        "def get_loader(data_path, coco_json_path, vocabulary, transform, batch_size, shuffle):\n",
        "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
        "    # COCO caption dataset\n",
        "    coco_dataser = CustomCocoDataset(data_path=data_path,\n",
        "                       coco_json_path=coco_json_path,\n",
        "                       vocabulary=vocabulary,\n",
        "                       transform=transform)\n",
        "\n",
        "    # Data loader for COCO dataset\n",
        "    # This will return (images, captions, lengths) for each iteration.\n",
        "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
        "    # captions: a tensor of shape (batch_size, padded_length).\n",
        "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
        "    custom_data_loader = torch.utils.data.DataLoader(dataset=coco_dataser,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=shuffle,\n",
        "                                              collate_fn=collate_function)\n",
        "    return custom_data_loader"
      ],
      "metadata": {
        "id": "pZzfnakdagDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Framing the model"
      ],
      "metadata": {
        "id": "crQLojDoKq60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, embedding_size):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(CNNModel, self).__init__()\n",
        "        resnet = models.resnet152(weights=True)\n",
        "        module_list = list(resnet.children())[:-1]      # delete the last fc layer.\n",
        "        self.resnet_module = nn.Sequential(*module_list)\n",
        "        self.linear_layer = nn.Linear(resnet.fc.in_features, embedding_size)\n",
        "        self.batch_norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, input_images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            resnet_features = self.resnet_module(input_images)\n",
        "        resnet_features = resnet_features.reshape(resnet_features.size(0), -1)\n",
        "        final_features = self.batch_norm(self.linear_layer(resnet_features))\n",
        "        return final_features\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_layer_size, vocabulary_size, num_layers, max_seq_len=20):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size)\n",
        "        self.lstm_layer = nn.LSTM(embedding_size, hidden_layer_size, num_layers, batch_first=True)\n",
        "        self.linear_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def forward(self, input_features, capts, lens):\n",
        "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "        embeddings = self.embedding_layer(caps)\n",
        "        embeddings = torch.cat((input_features.unsqueeze(1), embeddings), 1)\n",
        "        lstm_input = pack_padded_sequence(embeddings, lens, batch_first=True)\n",
        "        hidden_variables, _ = self.lstm_layer(lstm_input)\n",
        "        model_outputs = self.linear_layer(hidden_variables[0])\n",
        "        return model_outputs\n",
        "\n",
        "    def sample(self, input_features, lstm_states=None):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_indices = []\n",
        "        lstm_inputs = input_features.unsqueeze(1)\n",
        "        for i in range(self.max_seq_len):\n",
        "            hidden_variables, lstm_states = self.lstm_layer(lstm_inputs, lstm_states)          # hiddens: (batch_size, 1, hidden_size)\n",
        "            model_outputs = self.linear_layer(hidden_variables.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
        "            _, predicted_outputs = model_outputs.max(1)                        # predicted: (batch_size)\n",
        "            sampled_indices.append(predicted_outputs)\n",
        "            lstm_inputs = self.embedding_layer(predicted_outputs)                       # inputs: (batch_size, embed_size)\n",
        "            lstm_inputs = lstm_inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_indices = torch.stack(sampled_indices, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
        "        return sampled_indices"
      ],
      "metadata": {
        "id": "8L3x1dH8ah9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the trained looped data"
      ],
      "metadata": {
        "id": "R_r91irjKuur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Create model directory\n",
        "if not os.path.exists('models_dir/'):\n",
        "    os.makedirs('models_dir/')\n",
        "\n",
        "\n",
        "# Image preprocessing, normalization for the pretrained resnet\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "# Load vocabulary wrapper\n",
        "with open('data_dir/vocabulary.pkl', 'rb') as f:\n",
        "    vocabulary = pickle.load(f)\n",
        "\n",
        "\n",
        "# Build data loader\n",
        "custom_data_loader = get_loader('data_dir/resized_images', 'data_dir/annotations/captions_train2014.json', vocabulary,\n",
        "                         transform, 128,\n",
        "                         shuffle=True)\n",
        "\n",
        "# Build the models\n",
        "encoder_model = CNNModel(256).to(device)\n",
        "decoder_model = LSTMModel(256, 512, len(vocabulary), 1).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "parameters = list(decoder_model.parameters()) + list(encoder_model.linear_layer.parameters()) + list(encoder_model.batch_norm.parameters())\n",
        "optimizer = torch.optim.Adam(parameters, lr=0.001)\n",
        "\n",
        "\n",
        "# Train the models\n",
        "total_num_steps = len(custom_data_loader)\n",
        "for epoch in range(5):\n",
        "    for i, (imgs, caps, lens) in enumerate(custom_data_loader):\n",
        "\n",
        "        # Set mini-batch dataset\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        tgts = pack_padded_sequence(caps, lens, batch_first=True)[0]\n",
        "\n",
        "        # Forward, backward and optimize\n",
        "        feats = encoder_model(imgs)\n",
        "        outputs = decoder_model(feats, caps, lens)\n",
        "        loss = loss_criterion(outputs, tgts)\n",
        "        decoder_model.zero_grad()\n",
        "        encoder_model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print log info\n",
        "        if i % 10 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
        "                  .format(epoch, 5, i, total_num_steps, loss.item(), np.exp(loss.item())))\n",
        "\n",
        "        # Save the model checkpoints\n",
        "        if (i+1) % 1000 == 0:\n",
        "            torch.save(decoder_model.state_dict(), os.path.join(\n",
        "                'models_dir/', 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "            torch.save(encoder_model.state_dict(), os.path.join(\n",
        "                'models_dir/', 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))"
      ],
      "metadata": {
        "id": "9mefVAy1akC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_path = 'sample.jpg'\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def load_image(image_file_path, transform=None):\n",
        "    img = Image.open(image_file_path).convert('RGB')\n",
        "    img = img.resize([224, 224], Image.LANCZOS)\n",
        "\n",
        "    if transform is not None:\n",
        "        img = transform(img).unsqueeze(0)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "# Load vocabulary wrapper\n",
        "with open('data_dir/vocabulary.pkl', 'rb') as f:\n",
        "    vocabulary = pickle.load(f)\n",
        "\n",
        "\n",
        "# Build models\n",
        "encoder_model = CNNModel(256).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
        "decoder_model = LSTMModel(256, 512, len(vocabulary), 1)\n",
        "encoder_model = encoder_model.to(device)\n",
        "decoder_model = decoder_model.to(device)\n",
        "\n",
        "\n",
        "# Load the trained model parameters\n",
        "encoder_model.load_state_dict(torch.load('models_dir/encoder-2-3000.ckpt'))\n",
        "decoder_model.load_state_dict(torch.load('models_dir/decoder-2-3000.ckpt'))\n",
        "\n",
        "\n",
        "# Prepare an image\n",
        "img = load_image(image_file_path, transform)\n",
        "img_tensor = img.to(device)\n",
        "\n",
        "\n",
        "# Generate an caption from the image\n",
        "feat = encoder_model(img_tensor)\n",
        "sampled_indices = decoder_model.sample(feat)\n",
        "sampled_indices = sampled_indices[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
        "\n",
        "\n",
        "# Convert word_ids to words\n",
        "predicted_caption = []\n",
        "for token_index in sampled_indices:\n",
        "    word = vocabulary.i2w[token_index]\n",
        "    predicted_caption.append(word)\n",
        "    if word == '<end>':\n",
        "        break\n",
        "predicted_sentence = ' '.join(predicted_caption)\n",
        "\n",
        "\n",
        "# Print out the image and the generated caption\n",
        "print (predicted_sentence)\n",
        "img = Image.open(image_file_path)\n",
        "plt.imshow(np.asarray(img))"
      ],
      "metadata": {
        "id": "YuX_jBZEamRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BfpYZirMapvh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}